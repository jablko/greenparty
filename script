#!/usr/bin/env python3

import configparser
import csv
import difflib
import itertools
import operator
import re
import sys

from apiclient import discovery
import httplib2
from oauth2client import client
from oauth2client import tools
from oauth2client.file import Storage
import xlsxwriter
from xlsxwriter.utility import xl_range

# Take an export from Google Sheets and an export from Ecanvasser,
# 1) combine them into a single spreadsheet and
# 2) find groups of duplicate rows and merge them!
#
# The output is an Excel file so we can add some formatting to the data.

# From https://www.canadapost.ca/tools/pg/manual/PGaddress-e.asp#1449469
street_type_abbrs = {
    'abbey': 'Abbey',
    'acres': 'Acres',
    'allee': 'Allee',
    'alley': 'Alley',
    'autoroute': 'Aut',
    'aut': 'Aut',
    'avenue': 'Ave',
    'ave': 'Ave',
    'av': 'Av',
    'bay': 'Bay',
    'beach': 'Beach',
    'bend': 'Bend',
    'boulevard': 'Blvd',
    'blvd': 'Blvd',
    'boul': 'Boul',
    'by-pass': 'Bypass',
    'bypass': 'Bypass',
    'byway': 'Byway',
    'campus': 'Campus',
    'cape': 'Cape',
    'carre': 'Car',
    'car': 'Car',
    'carrefour': 'Carref',
    'carref': 'Carref',
    'centre': 'Ctr',
    'ctr': 'Ctr',
    'c': 'C',
    'cercle': 'Cercle',
    'chase': 'Chase',
    'chemin': 'Ch',
    'ch': 'Ch',
    'circle': 'Cir',
    'cir': 'Cir',
    'circuit': 'Circt',
    'circt': 'Circt',
    'close': 'Close',
    'common': 'Common',
    'concession': 'Conc',
    'conc': 'Conc',
    'corners': 'Crnrs',
    'crnrs': 'Crnrs',
    'cote': 'Cote',
    'cour': 'Cour',
    'cours': 'Cours',
    'court': 'Crt',
    'crt': 'Crt',
    'cove': 'Cove',
    'crescent': 'Cres',
    'cres': 'Cres',
    'croissant': 'Crois',
    'crois': 'Crois',
    'crossing': 'Cross',
    'cross': 'Cross',
    'cul-de-sac': 'Cds',
    'cds': 'Cds',
    'dale': 'Dale',
    'dell': 'Dell',
    'diversion': 'Divers',
    'divers': 'Divers',
    'downs': 'Downs',
    'drive': 'Dr',
    'dr': 'Dr',
    'echangeur': 'Ech',
    'ech': 'Ech',
    'end': 'End',
    'esplanade': 'Espl',
    'espl': 'Espl',
    'estates': 'Estate',
    'estate': 'Estate',
    'expressway': 'Expy',
    'expy': 'Expy',
    'extension': 'Exten',
    'exten': 'Exten',
    'farm': 'Farm',
    'field': 'Field',
    'forest': 'Forest',
    'freeway': 'Fwy',
    'fwy': 'Fwy',
    'front': 'Front',
    'gardens': 'Gdns',
    'gdns': 'Gdns',
    'gate': 'Gate',
    'glade': 'Glade',
    'glen': 'Glen',
    'green': 'Green',
    'grounds': 'Grnds',
    'grnds': 'Grnds',
    'grove': 'Grove',
    'harbour': 'Harbr',
    'harbr': 'Harbr',
    'heath': 'Heath',
    'heights': 'Hts',
    'hts': 'Hts',
    'highlands': 'Hghlds',
    'hghlds': 'Hghlds',
    'highway': 'Hwy',
    'hwy': 'Hwy',
    'hill': 'Hill',
    'hollow': 'Hollow',
    'ile': 'Ile',
    'impasse': 'Imp',
    'imp': 'Imp',
    'inlet': 'Inlet',
    'island': 'Island',
    'key': 'Key',
    'knoll': 'Knoll',
    'landing': 'Landng',
    'landng': 'Landng',
    'lane': 'Lane',
    'limits': 'Lmts',
    'lmts': 'Lmts',
    'line': 'Line',
    'link': 'Link',
    'lookout': 'Lkout',
    'lkout': 'Lkout',
    'loop': 'Loop',
    'mall': 'Mall',
    'manor': 'Manor',
    'maze': 'Maze',
    'meadow': 'Meadow',
    'mews': 'Mews',
    'montee': 'Montee',
    'moor': 'Moor',
    'mount': 'Mount',
    'mountain': 'Mtn',
    'mtn': 'Mtn',
    'orchard': 'Orch',
    'orch': 'Orch',
    'parade': 'Parade',
    'parc': 'Parc',
    'park': 'Pk',
    'pk': 'Pk',
    'parkway': 'Pky',
    'pky': 'Pky',
    'passage': 'Pass',
    'pass': 'Pass',
    'path': 'Path',
    'pathway': 'Ptway',
    'ptway': 'Ptway',
    'pines': 'Pines',
    'place': 'Pl',
    'pl': 'Pl',
    'plateau': 'Plat',
    'plat': 'Plat',
    'plaza': 'Plaza',
    'point': 'Pt',
    'pt': 'Pt',
    'pointe': 'Pointe',
    'port': 'Port',
    'private': 'Pvt',
    'pvt': 'Pvt',
    'promenade': 'Prom',
    'prom': 'Prom',
    'quai': 'Quai',
    'quay': 'Quay',
    'ramp': 'Ramp',
    'rang': 'Rang',
    'range': 'Rg',
    'rg': 'Rg',
    'ridge': 'Ridge',
    'rise': 'Rise',
    'road': 'Rd',
    'rd': 'Rd',
    'rond-point': 'Rdpt',
    'rdpt': 'Rdpt',
    'route': 'Rte',
    'rte': 'Rte',
    'row': 'Row',
    'rue': 'Rue',
    'ruelle': 'Rle',
    'rle': 'Rle',
    'run': 'Run',
    'sentier': 'Sent',
    'sent': 'Sent',
    'square': 'Sq',
    'sq': 'Sq',
    'street': 'St',
    'st': 'St',
    'subdivision': 'Subdiv',
    'subdiv': 'Subdiv',
    'terrace': 'Terr',
    'terr': 'Terr',
    'terrasse': 'Tsse',
    'tsse': 'Tsse',
    'thicket': 'Thick',
    'thick': 'Thick',
    'towers': 'Towers',
    'townline': 'Tline',
    'tline': 'Tline',
    'trail': 'Trail',
    'turnabout': 'Trnabt',
    'trnabt': 'Trnabt',
    'vale': 'Vale',
    'via': 'Via',
    'view': 'View',
    'village': 'Villge',
    'villge': 'Villge',
    'villas': 'Villas',
    'vista': 'Vista',
    'voie': 'Voie',
    'walk': 'Walk',
    'way': 'Way',
    'wharf': 'Wharf',
    'wood': 'Wood',
    'wynd': 'Wynd',
}

# From https://www.canadapost.ca/tools/pg/manual/PGaddress-e.asp#1442070
street_direction_abbrs = {
    'east': 'E',
    'e': 'E',
    'est': 'E',
    'north': 'N',
    'n': 'N',
    'nord': 'N',
    'northeast': 'NE',
    'ne': 'NE',
    'nord-est': 'NE',
    'northwest': 'NW',
    'nw': 'NW',
    'nord-ouest': 'NO',
    'no': 'NO',
    'south': 'S',
    's': 'S',
    'sud': 'S',
    'southeast': 'SE',
    'se': 'SE',
    'sud-est': 'SE',
    'southwest': 'SW',
    'sw': 'SW',
    'sud-ouest': 'SO',
    'so': 'SO',
    'west': 'W',
    'w': 'W',
    'ouest': 'O',
    'o': 'O',
}

# From https://www.canadapost.ca/tools/pg/manual/PGaddress-e.asp#1442119
unit_designator_abbrs = {
    'apartment': 'Apt',
    'apt': 'Apt',
    'appartement': 'App',
    'app': 'App',
    'suite': 'Suite',
    'bureau': 'Bureau',
    'unit': 'Unit',
    'unite': 'Unite',
}

abbreviations = [(re.compile(
    r'''
        (?<![0-9A-Z])
        ({abbrs})\.*
        (?![0-9A-Z])
    '''.format(abbrs='|'.join(re.escape(value) for value in abbrs)),
    re.IGNORECASE | re.VERBOSE), abbrs)
                 for abbrs in [
                     street_type_abbrs,
                     street_direction_abbrs,
                     unit_designator_abbrs,
                 ]]

# Normalize phone numbers and postal codes, to better find duplicate
# rows and merge duplicate values.
phone_number_re = re.compile('''
    (?<![0-9A-Z])
    1?[^0-9A-Z]*[0-9]{3}[^0-9A-Z]*[0-9]{3}[^0-9A-Z]*[0-9]{4}
    (?![0-9A-Z])
''', re.IGNORECASE | re.VERBOSE)
postal_code_re = re.compile('''
    (?<![0-9A-Z])
    [0-9A-Z]
    (?:
      [0-9O][0-9A-Z]
      (?:[^0-9A-Z]*[0-9O][0-9A-Z][0-9O])?
    )?
    (?![0-9A-Z])
''', re.IGNORECASE | re.VERBOSE)
house_unit = '[- 0-9A-Z]*?'
postal_box = '(?:PO[^0-9A-Z]*)?BOX[^0-9A-Z]*[0-9]*'
postal_box_re = re.compile(
    '''
        ^
        ({postal_box})
        [- ,/;]*
    '''.format(postal_box=postal_box),
    re.IGNORECASE | re.VERBOSE)
start_re = re.compile(
    r'''
        ^
        (?:({house_unit}(?=\ *[-,/;])|\#[0-9]*)[- ,/;]+)?
        ([0-9]+[A-Z]?)
        (?!
          [- ,/;]+
          (?:{street_type})\.*
          (?:[^0-9A-Z]+(?:{street_direction})\.*)?
          $
        )
        [- ,/;]+
    '''.format(
        house_unit=house_unit,
        street_type='|'.join(re.escape(value) for value in street_type_abbrs),
        street_direction='|'.join(
            re.escape(value) for value in street_direction_abbrs)),
    re.IGNORECASE | re.VERBOSE)
end_re = re.compile(
    '''
        [- ,/;]+
        ((?:{unit_designator})(?![A-Z])[^0-9A-Z]*{house_unit}|{postal_box})
        $
    '''.format(
        unit_designator='|'.join(
            re.escape(value) for value in unit_designator_abbrs),
        house_unit=house_unit,
        postal_box=postal_box),
    re.IGNORECASE | re.VERBOSE)
street_name_re = re.compile(
    r'''
        (?<![0-9A-Z])
        ({street_type})\.*
        (?:[^0-9A-Z]+({street_direction})\.*)?
        $
    '''.format(
        street_type='|'.join(re.escape(value) for value in street_type_abbrs),
        street_direction='|'.join(
            re.escape(value) for value in street_direction_abbrs)),
    re.IGNORECASE | re.VERBOSE)
ordinal_re = re.compile('''
    (?<![0-9A-Z])
    ([0-9]+)(?:st|nd|rd|th)
    (?![0-9A-Z])
''', re.IGNORECASE | re.VERBOSE)


def phone_number_cb(match):
  result = match.group(0)
  result = re.compile('[^0-9]').sub('', result)[-10:]
  result = result[:3] + '-' + result[3:6] + '-' + result[6:]
  return result


def postal_code_cb(match):
  result = match.group(0)
  result = re.compile('[^0-9A-Z]').sub('', result.upper())
  for i, char in enumerate(result):
    if char == '0O' [i % 2]:
      result = result[:i] + 'O0' [i % 2] + result[i + 1:]
  if len(result) == 6:
    result = result[:3] + ' ' + result[3:]
  return result


def street_name_cb(match):
  street_type, street_direction = match.groups()
  result = street_type_abbrs[street_type.lower()]
  if street_direction:
    result += ' ' + street_direction_abbrs[street_direction.lower()]
  return result


def parse_street_address(street_name):
  *parts, street_name = postal_box_re.split(street_name)
  if parts:
    adjacent, postal_box = parts
  else:
    postal_box = None
  *parts, street_name = start_re.split(street_name)
  if parts:
    adjacent, house_unit, house_number = parts
  else:
    house_number = None
    house_unit = None
  house_unit = ' '.join(value for value in [
      postal_box,
      house_unit,
  ] if value)
  if not house_unit:
    street_name = ' ' + street_name
    street_name, *parts = end_re.split(street_name)
    if parts:
      house_unit, adjacent = parts
    street_name = street_name[1:]
  street_name = street_name_re.sub(street_name_cb, street_name)
  return house_unit, house_number, street_name


def format_street_address(data):
  return '-'.join(value
                  for value in [
                      data['House Unit'],
                      ' '.join(value for value in [
                          data['House Number'],
                          data['Street Name'],
                      ] if value),
                  ] if value)


# This is for when we compare values. Convert them to lowercase, replace
# "&" with "and", strip all spaces around punctuation, etc. Basically
# strip away insignificant differences.
def norm(value):
  value = value.lower()
  for regex, abbrs in abbreviations:
    value = regex.sub(lambda match: abbrs[match.group(1)], value)
  value = ' ' + value + ' '
  value = re.compile('[^0-9A-Z]+', re.IGNORECASE).sub(' ', value)
  value = value.replace(' and ', ' ')
  for i, numeral in enumerate([
      'one',
      'two',
      'three',
      'four',
      'five',
      'six',
      'seven',
      'eight',
      'nine',
      'ten',
  ], 1):
    value = value.replace(' ' + numeral + ' ', ' {i} '.format(i=i))
  for i, ordinal in enumerate([
      'first',
      'second',
      'third',
      'fourth',
      'fifth',
      'sixth',
      'seventh',
      'eighth',
      'ninth',
      'tenth',
  ], 1):
    value = value.replace(' ' + ordinal + ' ', ' {i} '.format(i=i))
  value = ordinal_re.sub(r'\1', value)
  value = value.replace(' ', '')
  return value


# A group of duplicate rows. This is a class mostly because sets aren't
# hashable, and at the end of the day we want the *unique* groups of
# duplicate rows. Custom classes hash on id() (or something), which
# works for us!
class Dupes:

  def __init__(self, row):
    self.rows = {row}

  # Merge duplicate rows into a single row. Basically combine duplicate
  # information without throwing any information away. So if one row is
  # blank and another isn't, use the non-blank value, and if two rows
  # share the same value, use that value once. Do some fancy stuff with
  # substrings and capitalization.
  #
  # The result is a dictionary of lists. If a list has multiple values,
  # then we couldn't merge it -- and the caller can do with that what it
  # likes (join them with commas, etc.).
  #
  # Start with the unique values for each field. Find the values that
  # aren't substrings of any other values (e.g. "3 Comp 32" and "RR 2,
  # SITE 3 COMP 32"). Merge capitalization as follows: For each
  # character, if it's uppercase in any value, uppercase it in the
  # result (e.g. "MacDonald"), *unless* it's at the start of a word, in
  # which case if it's lowercase in any value that contains at least one
  # uppercase letter (i.e. isn't all lowercase), then lowercase it in
  # the result (e.g. "de Saeger"). Whew!
  def merge(self):
    names = set()
    for row in self.rows:
      names.update(row.data)
    result = {}
    for name in names:
      diff = {row.data.get(name) for row in self.rows if row.data.get(name)}
      if not diff.isdisjoint([
          'Both',
          'Kootenay Bay',
          'Membership: Annual',
          'Provincial',
          'Requested',
          'y a',
      ]):
        diff.discard('Yes')
      values = list({
          value.lower(): value
          for value in diff
          if not any(value.lower() in other_value.lower()
                     for other_value in diff
                     if value.lower() != other_value.lower())
      }.values())
      diff.difference_update(values)
      #if diff:
      #  from pprint import pprint
      #  pprint((diff, values))
      for i, value in enumerate(values):
        after = value
        for other_value in diff:
          if other_value.lower() != other_value:
            # Split at case-insensitive occurrences of other values,
            # merge capitalization and then put everything back
            # together.
            after, *parts = re.compile('(' + re.escape(other_value) + ')',
                                       re.IGNORECASE).split(after)
            for matching, adjacent in zip(parts[0::2], parts[1::2]):
              after += ''.join(
                  min(a, b) if value[j - 1:j].isalnum() else max(a, b)
                  if value.lower() != value else b
                  for j, a, b in zip(
                      itertools.count(
                          len(after)), matching, other_value)) + adjacent
        values[i] = after
      #if len(values) > 1:
      #  from pprint import pprint
      #  pprint(values)
      result[name] = values
    return result


# Gather rows into groups of duplicates.
#
# Basically, rows are duplicates if
# 1) the first names match and the last names, street addresses, and
#    postal codes are mergeable (blank or substrings of each other) and
# 3) any one of the following match: Email addresses, phone numbers,
#    street addresses, or postal codes.
#
# The first set of criteria (first names and mergeable) are "required"
# -- all are true or this isn't a duplicate. The second set (matching
# attributes) are "sufficient" -- any one suggests a likely duplicate
# (when combined with the "required" criteria). Each set is implemented
# as its own method, for convenience.
#
# Note that because rows are compared pairwise, A, B, and C are all
# grouped together if A and B and B and C are duplicates -- even if A
# and C aren't duplicates! This is mildly unfortunate (A and C could
# well -- and sometimes do -- have some attributes that aren't
# mergeable), but the only alternative that I could come up with would
# make the result indeterminate, depending what order the rows appear in
# (which seems *more* unfortunate?).
#
# First names get special treatment. There are lots of legitimate
# duplicates that are substrings of each other -- they're not identical
# -- (e.g. "Kenneth" and "Kenneth Arnold", etc.), and we want to catch
# these! But there are a whole lot of other rows belonging to couples
# (e.g. "Oscar" and "Annette & Oscar"), and we don't want to merge
# those, or we risk burying details about the individuals in the couple.
# So, what we do is presume that first names that contain "and" (or "&")
# belong to couples (it works well enough in practice). Then we say
# first names match only if both values contain, or neither value
# contains, "and".
#
# Comparing 30,000 rows pairwise is too slow (I tried!) so we index them
# by a bunch of attributes first. Then we look up all of the matching
# rows, in all of the indexes, and pairwise compare all of those,
# instead. Technically, if two rows' attributes are *all* substrings of
# each other (none are identical), then this isn't the same as pairwise
# comparing *every* row -- but that's a pretty rare and degenerate case.
class Row:

  registry = []
  by = {
      'EMAIL': {},
      'FIRST NAME': {},
      'LAST NAME': {},
      'HOME PHONE': {},
      'Cell': {},
      'Street': {},
  }

  def __init__(self, data):
    self.data = {}
    for name, value in data.items():
      value = ' '.join(value.split())
      if value:
        if name in [
            'HOME PHONE',
            'Cell',
        ]:
          #match = phone_number_re.match(value, 1)
          #if not match or match.end() != len(value) - 1:
          #  from pprint import pprint
          #  pprint(value)
          value = phone_number_re.sub(phone_number_cb, value)
        elif name == 'Postal code':
          #match = postal_code_re.match(value, 1)
          #if not match or match.end() != len(value) - 1:
          #  from pprint import pprint
          #  pprint(value)
          value = postal_code_re.sub(postal_code_cb, value)
        elif value.lower() == 'y':
          value = 'Yes'
        self.data[name] = value
    value = self.data.get('Street')
    if value:
      self.data['House Unit'], self.data['House Number'], self.data[
          'Street Name'] = parse_street_address(value)
      self.data['Street'] = format_street_address(self.data)

    self.dupes = Dupes(self)
    Row.registry.append(self)

    # This is the main de-duping code right here. We index rows by a
    # bunch of attributes, so for each index, look up all of the already
    # indexed rows (and by the way, index this row at the same time).
    # Then, for each row we just looked up, compare it to the current
    # row. Are they duplicates? If so, meld our associated groups of
    # duplicates together. The result is a list of every single row
    # (Row.registry), each one pointing to the group of duplicates it
    # belongs to (row.dupes) -- and from there we can get the unique
    # groups of duplicate rows, which was our goal. Yay!
    unique = set()
    for name, index in Row.by.items():
      value = self.data.get(name)
      if value:
        value = norm(value)
        result = index.get(value)
        if result is None:
          index[value] = [self]
        else:
          unique.update(result)
          result.append(self)
    for other_row in unique:
      if other_row not in self.dupes.rows and self.is_duplicate_required(
          other_row) and self.is_duplicate_sufficient(other_row):
        self.dupes.rows |= other_row.dupes.rows
        for dupes_row in other_row.dupes.rows:
          dupes_row.dupes = self.dupes

  def is_duplicate_required(self, other_row):
    value = self.data.get('FIRST NAME')
    if not value:
      return False
    other_value = other_row.data.get('FIRST NAME')
    if not other_value:
      return False
    value = value.replace('&', ' and ')
    value = value.lower()
    value = ' ' + value + ' '
    value = re.compile('[^0-9A-Z]+', re.IGNORECASE).sub(' ', value)
    other_value = other_value.replace('&', ' and ')
    other_value = other_value.lower()
    other_value = ' ' + other_value + ' '
    other_value = re.compile('[^0-9A-Z]+', re.IGNORECASE).sub(' ', other_value)
    if (' and ' in value) != (' and ' in other_value):
      return False
    value = norm(value)
    other_value = norm(other_value)
    if not (value in other_value or other_value in value):
      return False

    for name in [
        'LAST NAME',
        'Street',
        'Postal code',
    ]:
      value = self.data.get(name)
      if value:
        other_value = other_row.data.get(name)
        if other_value:
          value = norm(value)
          other_value = norm(other_value)
          if not (value in other_value or other_value in value):
            return False

    return True

  # Phone numbers and email addresses match if rows have a phone number
  # of email address in common (rows can potentially have multiple phone
  # numbers and multiple email addresses). Street addresses and postal
  # codes match if they're substrings of each other.
  def is_duplicate_sufficient(self, other_row):
    value = self.data.get('EMAIL')
    if value:
      other_value = other_row.data.get('EMAIL')
      if other_value:
        values = frozenset(norm(value).split(', '))
        other_values = frozenset(norm(other_value).split(', '))
        if not values.isdisjoint(other_values):
          return True

    values = set()
    other_values = set()
    for name in [
        'HOME PHONE',
        'Cell',
    ]:
      value = self.data.get(name)
      if value:
        values.update(norm(value).split(', '))
      other_value = other_row.data.get(name)
      if other_value:
        other_values.update(norm(other_value).split(', '))
    if not values.isdisjoint(other_values):
      return True

    for name in [
        'Street',
        'Postal code',
    ]:
      value = self.data.get(name)
      if value:
        other_value = other_row.data.get(name)
        if other_value:
          value = norm(value)
          other_value = norm(other_value)
          if value in other_value or other_value in value:
            return True

    return False


# Okay, welcome to the body of the script! Take two filenames, one for
# the Google Sheets export and the other for the Ecanvasser export, and
# parse them into rows.
config = configparser.ConfigParser()
config.read('config.ini')
spreadsheet_id = config['DEFAULT']['spreadsheet_id']
#spreadsheet, ecanvasser = sys.argv[1:]
ecanvasser, = sys.argv[1:]

#with open(spreadsheet) as f:
#  reader = csv.DictReader(f)
#  reader.fieldnames = fieldnames = [
#      ' '.join(name.split()) for name in reader.fieldnames
#  ]
#  for data in reader:
#    Row(data)
filename = 'credentials'
storage = Storage(filename)
credentials = storage.get()
if credentials is None:
  filename = 'client_secret.json'
  scope = 'https://www.googleapis.com/auth/spreadsheets'
  flow = client.flow_from_clientsecrets(filename, scope)
  flags = tools.argparser.parse_args([])
  credentials = tools.run_flow(flow, storage, flags)
http = credentials.authorize(httplib2.Http())
service = discovery.build('sheets', 'v4', http=http)

title = 'Sheet1'
result = service.spreadsheets().get(spreadsheetId=spreadsheet_id).execute()
for sheet in result['sheets']:
  properties = sheet['properties']
  if properties['title'] == title:
    break
sheet_id = properties['sheetId']
grid_properties = properties['gridProperties']
row_count = grid_properties['rowCount']
column_count = grid_properties['columnCount']
result = service.spreadsheets().values().get(
    spreadsheetId=spreadsheet_id,
    range=xl_range(0, 0, row_count - 1, column_count - 1)).execute()
fieldnames, *before = result['values']
fieldnames = [' '.join(name.split()) for name in fieldnames]
for data in before:
  Row(dict(zip(fieldnames, data)))

# This here is where we map Ecanvasser fields to our spreadsheet
# columns. Column names that are commented out don't have corresponding
# fields in Ecanvasser (I think?). Pretty much all of the Ecanvasser
# fields are captured in the spreadsheet (again, I think?).
with open(ecanvasser) as f:
  reader = csv.DictReader(f)
  for data in reader:
    data['EMAIL'] = data['Email Address']
    data['FIRST NAME'] = data['First Name']
    data['LAST NAME'] = data['Surname']
    #data['Email group Area'] =
    data['HOME PHONE'] = data['Home Phone']
    data['Cell'] = data['Mobile Phone']
    data['Volunteer Status'] = data['2017_volunteer_status']
    #data['Contact Notes'] = data['Notes - Canvassing']
    data['Street'] = format_street_address(data)
    data['Town'] = data['City']
    data['Postal code'] = data['Zip']
    #data['Wish to Donate'] =
    data['Donated Y /N'] = data['2017_donor']
    #data['Dontation Amount'] =
    #data['Thanked for Donation (email / phone / in person)'] =
    # This shimmy shamma puts the combined, unique values of three
    # Ecanvasser fields under one spreadsheet column.
    data['Support Level'] = ', '.join(
        sorted({
            data[name]
            for name in [
                'Party',
                '2017_party_support',
                '2017_support_level',
            ] if data[name]
        }))
    #data['Kims List'] =
    #data['Kims notes'] =
    #data['Follow-up (Kim Only)'] =
    #data['Followup (For Vol.)'] =
    data['DO NOT CALL'] = data['2017_do_not_call']
    data['Member'] = data['2017_member']
    data['Sign outside your home'] = data['2017_lawn_sign_request']
    #data['Door knocking'] =
    #data['Calling'] =
    #data['Street Can.'] =
    #data['Sign Install'] =
    #data['Data Entry'] =
    #data['Office Support'] =
    #data['Social Media & Comms'] =
    #data['Policy Research'] =
    #data['Graphic Design'] =
    #data['Coffee with Kim'] = data['Coffee With Kim']
    #data['Events planning'] =
    #data['Event set up/pack up'] =
    #data['Food (events)'] =
    #data['Poster put up'] =
    #data['Provide Billets & Food'] =
    #data['Letters to the editor'] =
    #data['Other skills/interests'] =
    #data['ED Polling stations'] =
    #data['ED - Provide lifts ED'] =
    #data['GOTV calling'] =
    #data['Scruitineering'] =
    #data['Other Committments'] =
    #data['Fundraising'] =
    Row(data)

frequencies = {
    ('Street Name', 'Street Name'): {},
    ('Town', 'Email group Area'): {},
}
for row in Row.registry:
  for (key, value), freqs_by in frequencies.items():
    key = row.data.get(key)
    value = row.data.get(value)
    if key and value:
      key = norm(key)
      freqs = freqs_by.get(key)
      if freqs is None:
        freqs_by[key] = freqs = {}
      if value in freqs:
        freqs[value] += 1
      else:
        freqs[value] = 1

for row in Row.registry:
  value = row.data.get('Street Name')
  if value:
    freqs = frequencies['Street Name', 'Street Name'][norm(value)]
    value, count = max(freqs.items(), key=operator.itemgetter(1))
    row.data['Street Name'] = value
    row.data['Street'] = format_street_address(row.data)

# Finally! The output. Get the unique groups of duplicate rows, merge
# them into single rows, and add them to the output.
unique = [
    Row.registry[i].dupes
    for i in sorted(
        {row.dupes: -1 - i
         for i, row in enumerate(reversed(Row.registry))}.values())
]
i = fieldnames.index('Street')
fieldnames[i:i + 1] = ['House Unit', 'House Number', 'Street Name']
i = fieldnames.index('Postal code')
fieldnames.insert(i + 1, 'Precinct')
i = fieldnames.index('Sign outside your home')
fieldnames.insert(i + 1, '2017_gotv_voted')
after = []
for dupes in unique:
  result = dupes.merge()
  values = result.get('Street')
  if values:
    result['House Unit'], result['House Number'], result['Street Name'] = (
        {value
         for value in values if value}
        for values in zip(*(parse_street_address(value) for value in values)))
  data = []
  for name in fieldnames:
    values = result.get(name)
    # When updating, values with no data are skipped. To clear data, use
    # an empty string ("").
    value = ''
    if values:
      value = ', '.join(sorted(values))
    elif name == 'Email group Area':
      values = result.get('Town')
      if values:
        freqs = frequencies['Town', 'Email group Area'].get(
            norm(', '.join(sorted(values))))
        if freqs is not None:
          value, count = max(freqs.items(), key=operator.itemgetter(1))
    data.append(value)
  while data and not data[-1]:
    data.pop()
  after.append(data)
requests = []
for tag, i1, j1, i2, j2 in reversed(
    difflib.SequenceMatcher(None, [tuple(data) for data in before],
                            [tuple(data) for data in after]).get_opcodes()):
  if tag != 'equal':
    if j2 - i2 > j1 - i1:
      requests.append(
          dict(insertDimension=dict(range=dict(
              sheetId=sheet_id,
              dimension='ROWS',
              startIndex=j1 + 1,
              endIndex=i1 + 1 + j2 - i2))))
    for data1, data2 in zip(before[i1:j1], after[i2:j2]):
      data2 += [''] * (len(data1) - len(data2))
    requests.append(
        dict(updateCells=dict(
            start=dict(
                sheetId=sheet_id, rowIndex=i1 + 1),
            rows=[
                dict(values=[
                    dict(userEnteredValue=dict(stringValue=value))
                    for value in data
                ]) for data in after[i2:j2]
            ],
            fields='userEnteredValue')))
    if j2 - i2 < j1 - i1:
      requests.append(
          dict(deleteDimension=dict(range=dict(
              sheetId=sheet_id,
              dimension='ROWS',
              startIndex=i1 + 1 + j2 - i2,
              endIndex=j1 + 1))))
if requests:
  result = service.spreadsheets().batchUpdate(
      spreadsheetId=spreadsheet_id, body=dict(requests=requests)).execute()
#with xlsxwriter.Workbook('spreadsheet.xlsx') as workbook:
#  worksheet = workbook.add_worksheet()
#  worksheet.write_row(0, 0, fieldnames)
#  for i, data in enumerate(after):
#    worksheet.write_row(i + 1, 0, data)
#
#  # The rest, from here to the end, is formatting. Make the output look
#  # roughly like the existing spreadsheet.
#  worksheet.freeze_panes(1, 0)
#  default_props = dict(font_size=12, text_wrap=True)
#  default = workbook.add_format(default_props)
#  strong = workbook.add_format(dict(default_props, bold=True))
#  worksheet.set_column(0, len(fieldnames) - 1, None, default)
#  worksheet.set_row(0, None, strong)
#  for name in [
#      'EMAIL',
#      #'Street',
#  ]:
#    i = fieldnames.index(name)
#    worksheet.set_column(i, i, 24)
#  for name in [
#      'FIRST NAME',
#      'LAST NAME',
#  ]:
#    i = fieldnames.index(name)
#    worksheet.set_column(i, i, 16, strong)
#  for name in [
#      'Email group Area',
#      'Town',
#      'Postal code',
#  ]:
#    i = fieldnames.index(name)
#    worksheet.set_column(i, i, 12)
#  for name in [
#      'HOME PHONE',
#      'Cell',
#      'Volunteer Status',
#      'Street Name',
#      'Wish to Donate',
#      'Donated Y /N',
#      'Dontation Amount',
#      'Thanked for Donation (email / phone / in person)',
#      'Support Level',
#      'Kims List',
#      'Kims notes',
#      'Follow-up (Kim Only)',
#      'Followup (For Vol.)',
#      'DO NOT CALL',
#      'Member',
#  ]:
#    i = fieldnames.index(name)
#    worksheet.set_column(i, i, 16)
#  for name in ['Contact Notes']:
#    i = fieldnames.index(name)
#    worksheet.set_column(i, i, 32)
#  #for name in [
#  #    'House Unit',
#  #    'House Number',
#  #    'Precinct',
#  #    'Sign outside your home',
#  #    '2017_gotv_voted',
#  #    'Door knocking',
#  #    'Calling',
#  #    'Street Can.',
#  #    'Sign Install',
#  #    'Data Entry',
#  #    'Office Support',
#  #    'Social Media & Comms',
#  #    'Policy Research',
#  #    'Graphic Design',
#  #    'Coffee with Kim',
#  #    'Events planning',
#  #    'Event set up/pack up',
#  #    'Food (events)',
#  #    'Poster put up',
#  #    'Provide Billets & Food',
#  #    'Letters to the editor',
#  #    'Other skills/interests',
#  #    'ED Polling stations',
#  #    'ED - Provide lifts ED',
#  #    'GOTV calling',
#  #    'Scruitineering',
#  #    'Other Committments',
#  #    'Fundraising',
#  #]:
